{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                         GTFS real-time downnloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install gtfs-realtime-bindings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gtfs-realtime-bindings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install protobuf3-to-dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.transit import gtfs_realtime_pb2\n",
    "import urllib\n",
    "import urllib.request\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import multiprocessing\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from zipfile import ZipFile\n",
    "from pyproj import Geod\n",
    "from google.transit import gtfs_realtime_pb2\n",
    "import requests\n",
    "from google.transit import gtfs_realtime_pb2\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "from google.protobuf.json_format import MessageToJson\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import datetime\n",
    "from protobuf_to_dict import protobuf_to_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining some methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgs84_geod = Geod(ellps='WGS84') #Distance will be measured on this ellipsoid - more accurate than a spherical method\n",
    "def monthname(mydate):\n",
    "    mydate = datetime.datetime.now()\n",
    "    m = mydate.strftime(\"%B\")\n",
    "    return(m)\n",
    "\n",
    "def Brisbane(epoch):\n",
    "    a = time.strftime(\"%d-%m-%Y %H:%M:%S\", time.localtime(epoch))\n",
    "    return(a)\n",
    "\n",
    "def createCSVfile(inputlist , name):\n",
    "    with open( name , 'w') as f:\n",
    "        for i in inputlist:\n",
    "            k = 0\n",
    "            for item in i:  \n",
    "                f.write(str(item) + ',')\n",
    "                k = k+1\n",
    "            f.write('\\n')\n",
    "        return(f)\n",
    "def inputCSVfile(csvfile):\n",
    "    list1= []\n",
    "    with open(csvfile, 'r') as f:\n",
    "        for i in f:\n",
    "            j = i.split(',')\n",
    "            \n",
    "            le = len(j)\n",
    "            j[le - 1] = (j[le- 1]).strip()\n",
    "            list1.append(j)\n",
    "        return(list1)\n",
    "\n",
    "def createCSVfileWCD(inputlist , name):\n",
    "    with open( name , 'w') as f:\n",
    "        for i in inputlist:\n",
    "            f.write(str(i))\n",
    "            f.write('\\n')\n",
    "        return(f)\n",
    "def Distance(lat1,lon1,lat2,lon2):\n",
    "    az12 ,az21,dist = wgs84_geod.inv(lon1,lat1,lon2,lat2) #Yes, this order is correct\n",
    "    return (dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Working_Directory = r\"C:\\Users\\n10680535\\OneDrive - Queensland University of Technology\\Shubham Sharma\\Research\\2. Codes\\1. GTFS Data Downloader/\"   #ensure slash sign in the end \"/\" not \"\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create sub-folders to store different files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Static_Folder = Working_Directory + 'GTFS Static/'\n",
    "Realtime_folder  = Working_Directory + 'GTFS Realtime/'\n",
    "if not os.path.exists(Static_Folder):\n",
    "    os.makedirs(Static_Folder)\n",
    "if not os.path.exists(Realtime_folder):\n",
    "    os.makedirs(Realtime_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTFS-realtime download link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtfs_realtime_link = r'https://gtfsrt.api.translink.com.au/Feed/SEQ'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTFS- static download link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtfs_static_link = r\"https://gtfsrt.api.translink.com.au/GTFS/SEQ_GTFS.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading static and real-time feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\n10680535\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-8ddc1c6571fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[0mkt3_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkt3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrival\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[0mkt3_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'arrival_delay'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'arrival_time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'arrival_uncertainty'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[0mkt3_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkt3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeparture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m     \u001b[0mkt3_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'departure_delay'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'departure_time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'departure_uncertainty'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[0mkt3_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkt3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkt3_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkt3_2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4215\u001b[0m             \u001b[1;31m# GH 25959 use pd.array instead of tolist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4216\u001b[0m             \u001b[1;31m# so extension arrays can be used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4217\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor_expanddim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4218\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4219\u001b[0m             return self._constructor(mapped, index=self.index).__finalize__(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    507\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mis_named_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m                         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 509\u001b[1;33m                     \u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    510\u001b[0m                     \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m         return _list_of_series_to_arrays(\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoerce_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m         )\n\u001b[0;32m    533\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m_list_of_series_to_arrays\u001b[1;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m         \u001b[0maligned_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malgorithms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    597\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maligned_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, out, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m   1711\u001b[0m         \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1712\u001b[0m     )\n\u001b[1;32m-> 1713\u001b[1;33m     \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1715\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mflip_order\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t1 = 0\n",
    "datee = datetime.datetime.strptime(str(Brisbane(t1)),\"%d-%m-%Y %H:%M:%S\" )\n",
    "m = datee.month\n",
    "y = datee.year\n",
    "d = datee.day\n",
    "m_name = monthname(Brisbane(t1))\n",
    "date_name1 = str(d)+\"-\"+str(m)+\"-\"+str(y)\n",
    "index_o = 0\n",
    "while True:\n",
    "    #Directories for a new Day\n",
    "    t = int(time.time())\n",
    "    datee = datetime.datetime.strptime(str(Brisbane(t)),\"%d-%m-%Y %H:%M:%S\" )\n",
    "    m = datee.month\n",
    "    y = datee.year\n",
    "    d = datee.day\n",
    "    m_name = monthname(Brisbane(t))\n",
    "    date_name = str(d)+\"-\"+str(m)+\"-\"+str(y)\n",
    "    Month_year = str(m_name)+\" ,\"+str(y)\n",
    "    if date_name!= date_name1:\n",
    "        index_o = 0\n",
    "        datee = datetime.datetime.strptime(str(Brisbane(t)),\"%d-%m-%Y %H:%M:%S\" )\n",
    "        m = datee.month\n",
    "        y = datee.year\n",
    "        d = datee.day\n",
    "        m_name = monthname(Brisbane(t))\n",
    "        date_name = str(d)+\"-\"+str(m)+\"-\"+str(y)\n",
    "        Month_year = str(m_name)+\" ,\"+str(y)\n",
    "        TUdirectory = Realtime_folder + \"TripUpdate entity\" +\"/\" + 'TU '+str(Month_year)\n",
    "        VPdirectory = Realtime_folder + \"VehiclePosition entity\" +\"/\" + 'VP '+str(Month_year)\n",
    "        if not os.path.exists(TUdirectory):\n",
    "            os.makedirs(TUdirectory)\n",
    "        TUdirectory2= TUdirectory+ \"/\"+ 'TU '+str(date_name)\n",
    "        if not os.path.exists(TUdirectory2):\n",
    "            os.makedirs(TUdirectory2) \n",
    "        TUdirectory3= TUdirectory2+ \"/\"+ 'TU feeds '+str(date_name)\n",
    "        TUdirectory4= TUdirectory2+ \"/\"+ 'TU Speed Analysis'+str(date_name)\n",
    "        if not os.path.exists(TUdirectory3):\n",
    "            os.makedirs(TUdirectory3)\n",
    "        if not os.path.exists(TUdirectory4):\n",
    "            os.makedirs(TUdirectory4)\n",
    "        if not os.path.exists(VPdirectory):\n",
    "            os.makedirs(VPdirectory)\n",
    "        VPdirectory2= VPdirectory+ \"/\"+ 'VP '+str(date_name)\n",
    "        if not os.path.exists(VPdirectory2):\n",
    "            os.makedirs(VPdirectory2)\n",
    "            \n",
    "        #Downloading and processing static file\n",
    "        \n",
    "        GTFS_Static = urllib.request.urlretrieve(gtfs_static_link, Static_Folder + '/GTFS Static ' +date_name + '.zip')\n",
    "        zip_file = ZipFile(Static_Folder + '/GTFS Static ' +date_name + '.zip')\n",
    "        trips = pd.read_csv(zip_file.open('trips.txt'))\n",
    "        stop_times = pd.read_csv(zip_file.open('stop_times.txt'))\n",
    "        shapes = pd.read_csv(zip_file.open('shapes.txt'))\n",
    "        shapes['shape_pt_sequence'] = shapes['shape_pt_sequence'].astype(str)\n",
    "        Route_Shape_stop = stop_times.merge(trips, on = 'trip_id', how = 'left')\n",
    "        Route_Shape_stop = Route_Shape_stop[['shape_id', 'stop_id', 'stop_sequence', 'route_id', 'trip_id']].drop_duplicates(keep = 'first')\n",
    "        Route_Shape_stop['stop_sequence'] = Route_Shape_stop['stop_sequence'].astype(int)\n",
    "        Route_Shape_stop['stop_id'] = Route_Shape_stop['stop_id'].astype(str)\n",
    "        shapes2 = shapes.copy(deep = True)\n",
    "        shapes2['shape_pt_sequence_next'] = (shapes2['shape_pt_sequence'].astype(int) + 1).astype(str)\n",
    "        shapes2['stop_seq_1'] = (shapes2['shape_pt_sequence'].astype(int)/10000).astype(int)\n",
    "        shapes2['stop_seq_2'] = (shapes2['shape_pt_sequence_next'].astype(int)/10000).astype(int)\n",
    "        shapes3 = shapes2.merge(shapes, left_on = ['shape_id', 'shape_pt_sequence_next'], right_on = ['shape_id', 'shape_pt_sequence'], how = 'left')\n",
    "        shapes3['distance'] = Distance(shapes3['shape_pt_lat_x'].tolist(),shapes3['shape_pt_lon_x'].tolist(),shapes3['shape_pt_lat_y'].tolist(),shapes3['shape_pt_lon_y'].tolist())\n",
    "        shapes3['distance'] = shapes3['distance']/1000\n",
    "        shapes3 = shapes3.loc[shapes3['stop_seq_1'] == shapes3['stop_seq_2']]\n",
    "        shapes3['stop_seq_2'] = shapes3['stop_seq_2'] + 1\n",
    "        shapes4 = pd.DataFrame(shapes3.groupby(['shape_id', 'stop_seq_1', 'stop_seq_2'], as_index = False)['distance'].sum())\n",
    "        distance_file = shapes4\n",
    "        date_name1 = date_name\n",
    "        \n",
    "    #Downloading the feed\n",
    "    \n",
    "    feed = gtfs_realtime_pb2.FeedMessage()\n",
    "    response = urllib.request.urlopen('https://gtfsrt.api.translink.com.au/Feed/SEQ')\n",
    "    feed.ParseFromString(response.read())\n",
    "    dict_obj = protobuf_to_dict(feed)\n",
    "    r = json.dumps(dict_obj)\n",
    "    loaded_r = json.loads(r)\n",
    "    kppa = pd.DataFrame(loaded_r['entity'])\n",
    "    kt = kppa.trip_update.apply(pd.Series)\n",
    "    kt = pd.concat([kppa, kt], axis= 1)\n",
    "    kt2 = kt['stop_time_update'].apply(pd.Series).reset_index().melt(id_vars='index').dropna()[['index', 'value']].set_index('index')\n",
    "    kt2 = kt.merge(kt2, left_index=True, right_index=True, how = 'left')\n",
    "    kt2.columns = [ 'id', 'trip_update', 'vehicle', 0 , 'stop_time_update', 'timestamp', 'trip','vehicle2', 'value']\n",
    "    kt3 = kt2.value.apply(pd.Series)\n",
    "    kt3_1 = kt3.arrival.apply(pd.Series)\n",
    "    kt3_1.columns = ['0', 'arrival_delay', 'arrival_time', 'arrival_uncertainty']\n",
    "    kt3_2 = kt3.departure.apply(pd.Series)\n",
    "    kt3_2.columns = ['0', 'departure_delay', 'departure_time', 'departure_uncertainty']\n",
    "    kt3_f = pd.concat([kt3, kt3_1,kt3_2], axis = 1)\n",
    "    kt4 = kt2.trip.apply(pd.Series)\n",
    "    kt5 = kt2.vehicle2.apply(pd.Series)\n",
    "    kt5.columns = [0, 'vehicle_id']\n",
    "    KT_F = pd.concat([kt2, kt3_f, kt4, kt5], axis=1)\n",
    "    KT_F = KT_F[['trip_id', 'start_time', 'start_date', 'route_id', 'stop_id', 'stop_sequence', 'arrival_delay', 'arrival_time', 'arrival_uncertainty','departure_delay' , 'departure_time',  'departure_uncertainty', 'schedule_relationship', 'vehicle_id', 'timestamp']]\n",
    "    KT_F = KT_F.loc[~KT_F['timestamp'].isna()]\n",
    "    KT_F = KT_F.loc[:,~KT_F.columns.duplicated()]\n",
    "    KT_F = KT_F.merge(Route_Shape_stop, on = ['stop_sequence', 'stop_id', 'route_id', 'trip_id'], how = 'left')\n",
    "    KT_F.to_csv(TUdirectory3 + '/TU feed'+ date_name + ' '+ str(t)  + '.csv')\n",
    "    KT_F_copy = KT_F.copy(deep = True).loc[~KT_F['stop_sequence'].isna()][['shape_id', 'trip_id', 'arrival_time', 'departure_time', 'stop_id', 'stop_sequence']]\n",
    "    KT_F_copy['stop_sequence2'] =  KT_F_copy['stop_sequence']-1\n",
    "    KT_F_copy = KT_F_copy.merge(KT_F_copy, left_on = ['trip_id', 'stop_sequence'], right_on = ['trip_id', 'stop_sequence2'], how = 'inner')\n",
    "    KT_F_copy[['stop_sequence_x', 'stop_sequence_y']] = KT_F_copy[['stop_sequence_x', 'stop_sequence_y']].astype(int)\n",
    "    distance_file[['stop_seq_1', 'stop_seq_2']] = distance_file[['stop_seq_1', 'stop_seq_2']].astype(int)\n",
    "    #distance_file['shape_id'] = distance_file['shape_id'].astype(str)\n",
    "    KT_F_copy = KT_F_copy.merge(distance_file, left_on = ['shape_id_x', 'stop_sequence_x', 'stop_sequence_y'], right_on = ['shape_id', 'stop_seq_1', 'stop_seq_2'], how = 'inner')\n",
    "    KT_F_copy['Travel_Speed_DtoA'] = 3600*KT_F_copy['distance']/(KT_F_copy['arrival_time_y'] - KT_F_copy['departure_time_x'])\n",
    "    KT_F_copy[['shape_id_x', 'trip_id', 'stop_id_x', 'stop_sequence_x', 'stop_id_y', 'stop_sequence_y','distance', 'Travel_Speed_DtoA']].to_csv(TUdirectory4 + '/TU Speed'+ date_name + ' '+ str(t)  + '.csv')\n",
    "    vt = kppa.vehicle.apply(pd.Series)\n",
    "    vt1 = vt.position.apply(pd.Series)\n",
    "    vt2 = vt.trip.apply(pd.Series)\n",
    "    vt4 = vt.vehicle.apply(pd.Series)\n",
    "    VE = pd.concat([vt, vt1, vt2, vt4], axis=1)\n",
    "    VE = VE[['current_status', 'stop_id', 'timestamp', 'trip_id','latitude', 'longitude' , 'route_id','id', 'label' ]]\n",
    "    VE = VE.loc[~VE['timestamp'].isna()]\n",
    "    VE.to_csv(VPdirectory2 + '/VP feed '+ date_name + ' '+ str(t)  + '.csv')\n",
    "    index_o = index_o + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
